{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Loading the data \n",
    "\n",
    "path=\"/home/affine/Downloads/train_1.csv\"\n",
    "\n",
    "#creating a dataframe by loading the path of inputfile by using databricks library for csv files\n",
    "train = sqlCtx.read.format('com.databricks.spark.csv').options(header='true', inferschema='true',delimiter=',').load(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## removing duplicates\n",
    "train = train.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##saving the data \n",
    "df = train.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##addition of extra columns (derived from the column present in the data)\n",
    "from pyspark.sql.functions import month,datediff\n",
    "\n",
    "df1 = df.withColumn(\"month\", month(df.date_time))\n",
    "df1 = df1.withColumn(\"los\",datediff(df1.srch_co,df1.srch_ci))\n",
    "df1 = df1.withColumn(\"bw\",datediff(df1.srch_ci,df1.date_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##removing unnecessary cols\n",
    "df1 = df1.select([col for col in df1.columns if col not in {\"hotel_market\",\"user_location_region\",\"user_location_city\",\"hotel_cluster\",\"srch_destination_id\",\"user_id\",\"cnt\",\"srch_ci\",\"srch_co\",\"date_time\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4999"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##removing records with null values\n",
    "df1 = df1.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2867"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##changing the data type to double of all the columns\n",
    "from pyspark.sql.types import *\n",
    "df2 = df1.withColumn(\"site_name_1\",df1.site_name.cast(DoubleType())).\\\n",
    "        withColumn(\"posa_continent_1\",df1.posa_continent.cast(DoubleType())).\\\n",
    "        withColumn(\"user_location_country_1\",df1.user_location_country.cast(DoubleType())).\\\n",
    "        withColumn(\"is_mobile_1\",df1.is_mobile.cast(DoubleType())).\\\n",
    "        withColumn(\"is_package_1\",df1.is_package.cast(DoubleType())).\\\n",
    "        withColumn(\"channel_1\",df1.channel.cast(DoubleType())).\\\n",
    "        withColumn(\"srch_adults_cnt_1\",df1.srch_adults_cnt.cast(DoubleType())).\\\n",
    "        withColumn(\"srch_children_cnt_1\",df1.srch_children_cnt.cast(DoubleType())).\\\n",
    "        withColumn(\"srch_rm_cnt_1\",df1.srch_rm_cnt.cast(DoubleType())).\\\n",
    "        withColumn(\"srch_destination_type_id_1\",df1.srch_destination_type_id.cast(DoubleType())).\\\n",
    "        withColumn(\"is_booking_1\",df1.is_booking.cast(DoubleType())).\\\n",
    "        withColumn(\"hotel_continent_1\",df1.hotel_continent.cast(DoubleType())).\\\n",
    "        withColumn(\"hotel_country_1\",df1.hotel_country.cast(DoubleType())).\\\n",
    "        withColumn(\"month_1\",df1.month.cast(DoubleType())).\\\n",
    "        withColumn(\"los_1\",df1.los.cast(DoubleType())).\\\n",
    "        withColumn(\"bw_1\",df1.bw.cast(DoubleType())).\\\n",
    "        withColumn(\"orig_destination_distance_1\",df1.orig_destination_distance.cast(DoubleType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##removing repetative columns\n",
    "df2 = df2.select([cols for cols in df2.columns if cols not in {\"site_name\",\"posa_continent\",\"user_location_country\",\"is_mobile\",\"is_package\",\"channel\",\"srch_adults_cnt\",\"srch_children_cnt\",\"srch_rm_cnt\",\"srch_destination_type_id\",\"is_booking\",\"hotel_continent\",\"hotel_country\",\"month\",\"los\",\"bw\",\"orig_destination_distance\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Changing spark dataframe to pandas to convert it to matrix\n",
    "df_pandas = df2.toPandas()\n",
    "df_matrix = df_pandas.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.00000000e+00,   3.00000000e+00,   6.60000000e+01, ...,\n",
       "          4.00000000e+00,   1.60000000e+01,   2.23426410e+03],\n",
       "       [  2.00000000e+00,   3.00000000e+00,   6.60000000e+01, ...,\n",
       "          5.00000000e+00,   1.06000000e+02,   9.13625900e+02],\n",
       "       [  2.00000000e+00,   3.00000000e+00,   6.60000000e+01, ...,\n",
       "          5.00000000e+00,   1.06000000e+02,   9.11514200e+02],\n",
       "       ..., \n",
       "       [  2.00000000e+00,   3.00000000e+00,   6.60000000e+01, ...,\n",
       "          3.00000000e+00,   5.00000000e+01,   8.51538400e+02],\n",
       "       [  2.00000000e+00,   3.00000000e+00,   6.60000000e+01, ...,\n",
       "          2.00000000e+00,   6.00000000e+01,   1.06299200e+02],\n",
       "       [  2.00000000e+00,   3.00000000e+00,   6.60000000e+01, ...,\n",
       "          2.00000000e+00,   6.00000000e+01,   1.04421200e+02]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/affine/Applications/spark-1.6/python/pyspark/mllib/clustering.py:176: UserWarning: Support for runs is deprecated in 1.6.0. This param will have no effect in 1.7.0.\n",
      "  \"Support for runs is deprecated in 1.6.0. This param will have no effect in 1.7.0.\")\n"
     ]
    }
   ],
   "source": [
    "##Making clustering model\n",
    "\n",
    "clusters = KMeans.train(sc.parallelize(df_matrix), 6, maxIterations=10,\n",
    "        runs=10, initializationMode=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##number of clusters\n",
    "clusters.k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##cluster indexes for the data to be predicted\n",
    "cluster_ind = clusters.predict(sc.parallelize(df_matrix))\n",
    "x = cluster_ind.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([(0, 67), (1, 635), (2, 175), (3, 811), (4, 318), (5, 861)])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##size of each cluster\n",
    "cluster_sizes = cluster_ind.countByValue().items()\n",
    "cluster_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "819049.6719431345"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##calculating the within set sum of square error (helps in tuning the model)\n",
    "def error(point):\n",
    "    center = clusters.centers[clusters.predict(point)]\n",
    "    return sqrt(sum([x**2 for x in (point - center)]))\n",
    "\n",
    "WSSSE = sc.parallelize(df_matrix).map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "\n",
    "WSSSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  8.80597015e+00,   3.25373134e+00,   1.16507463e+02,\n",
       "         1.94029851e-01,   5.97014925e-02,   4.17910448e+00,\n",
       "         2.04477612e+00,   4.32835821e-01,   1.11940299e+00,\n",
       "         2.53731343e+00,   1.79104478e-01,   2.64179104e+00,\n",
       "         6.19850746e+01,   8.07462687e+00,   2.22388060e+00,\n",
       "         7.82388060e+01,   8.10413414e+03])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##centroid coordinates of clusters\n",
    "Cluster_Centers=clusters.clusterCenters\n",
    "Cluster_Centers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##saving the model\n",
    "path_save = \"/home/affine/Downloads\"\n",
    "clusters.save(sc, path_save)\n",
    "\n",
    "###command for retreiving the model\n",
    "Model = KMeansModel.load(sc, path_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###creating final matrix with cluster assigned to them\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "clust_ind = np.asarray(x).reshape(df_matrix.shape[0],1)\n",
    "\n",
    "final = np.append(df_matrix,clust_ind , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##converting matrix to dataframe\n",
    "index = list(range(df_matrix.shape[0]))\n",
    "columns = df2.columns + ['cluster']   \n",
    "values = final\n",
    "\n",
    "df_final = pd.DataFrame(values, index=index, columns=columns)\n",
    "df_spark = sqlContext.createDataFrame(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
